{
  "训练参数调节 | SD 训练 UI": "Training Parameters Tuning | SD Training UI",
  "SD-Trainer": "SD-Trainer",
  "LoRA训练": "LoRA Training",
  "新手": "Beginner",
  "专家": "Expert",
  "工具": "Tools",
  "参数详解": "Parameter Explanation",
  "Tensorboard": "Tensorboard",
  "WD 1.4 标签器": "WD 1.4 Tagger",
  "其他": "Other",
  "UI 设置": "UI Settings",
  "关于": "About",
  "Github": "Github",
  "灯泡": "Bulb",
  "#": "",
  "训练参数调节": "Training Parameters Tuning",
  "设置训练用模型、数据集": "Set Training Models and Datasets",
  "底模选择": "Base Model Selection",
  "底模，尽量选祖宗级别的模型练出来的LoRA会更通用。如果在融合模型上训练可能会": "Base model, preferably choose ancestral models for more versatile LoRA training. Training on fusion models may result in",
  "仅仅在你训练的底模上生成图片拥有不错的效果": "better image generation, but with reduced generalization. You can make the choice.",
  "但是失去了通用性。可以自己抉择": "",
  "什么是祖宗级别的模型？": "What are ancestral models?",
  "sd1.5 2.0、novelai 原版泄露模型。也就是非融合模型。融合模型比如 anything 系列融合了一大堆，orangemix系列融合了 anything 和 basil 更灵车了等等。在他们上面训练的会迁移性更差一些。": "sd1.5 2.0, novelai original leaked models. These are non-fusion models. Fusion models like the anything series fuse many models, and orangemix series fuse anything and basil for better performance. Training on them may have worse transferability.",
  "训练分辨率": "Training Resolution",
  "训练时的分辨率": "Resolution during training",
  "宽,高": "width, height",
  "，可以是非正方形，但必须为64的整数倍。建议使用大于 512x512 且小于 1024x1024 的值，长宽比根据训练集的占比决定，一般来说方形的可以照顾到各种不同的分辨率。如果多数为长图可以使用512x768这种分辨率，如果宽图居多则可以使用768x512等。": ", can be non-square, but must be a multiple of 64. It's recommended to use values greater than 512x512 and less than 1024x1024. The aspect ratio should be determined by the distribution of the training set. For instance, if long images are dominant, consider using resolutions like 512x768, or if wide images dominate, use 768x512.",
  "ARB 桶": "ARB Buckets",
  "默认开启 ARB 桶，以允许使用非固定宽高比的图像来训练（简单来说就是不需要手动剪裁了）。ARB 桶在一定程度上会增加训练时间。": "ARB buckets are enabled by default, allowing the use of images with non-fixed aspect ratios for training (essentially, no need for manual cropping). ARB buckets may increase training time to some extent.",
  "ARB桶分辨率必须大于训练分辨率": "ARB bucket resolution must be higher than the training resolution.",
  "学习率与优化器设置": "Learning Rate and Optimizer Settings",
  "学习率设置": "Learning Rate Settings",
  "UNet和TE的学习率通常是不同的，因为学习难度不同，通常UNet的学习率会比TE高 。": "UNet and TE usually have different learning rates, as their learning difficulties differ. Generally, UNet's learning rate is higher than TE's.",
  "如图所示，我们希望UNet和TE都处于一个恰好的位置（绿色部分），但是这个值我们不知道。": "As shown in the graph, we aim for UNet and TE to be in an optimal range (green portion), but we're uncertain about the exact values.",
  "如果UNet训练不足，那么生成的图会不像，UNet训练过度会导致面部扭曲或者产生大量色块。TE训练不足会让出图对Prompt的服从度低，TE训练过度则会生成多余的物品。": "Insufficient UNet training results in unfaithful images, while excessive training leads to distorted faces or excessive color patches. Inadequate TE training reduces adherence to prompts in output images, whereas excessive training generates superfluous objects.",
  "总学习步数 = （图片数量 * 重复次数 * epoch）/ 批次大小": "Total Learning Steps = (Number of Images * Repetitions * Epochs) / Batch Size",
  "以UNet学习率为1e-4为例，一般来说图片较少的时候训练人物需要至少1000步，训练画风则需要至少2500步，训练概念则需要至少3000步。这里只是最低的步数，图片多则需要更多步数。学习率更大可以适当减少步数，但并非线性关系，使用两倍的学习率需要使用比之前步数的一半更多的步数。": "Taking UNet's learning rate as 1e-4, generally, for fewer images, training characters requires at least 1000 steps, style training needs at least 2500 steps, and concept training needs at least 3000 steps. These are just minimum steps; more images require more steps. With a larger learning rate, steps can be slightly reduced. However, it's not linear; using double the learning rate requires more than half the previous steps.",
  "决定学习率和步数的最好方法是先训练，再测试。一般比较好的初始值为UNet使用1e-4，TE使用5e-5": "The best way to determine learning rates and steps is to train first, then test. Good initial values are typically 1e-4 for UNet and 5e-5 for TE.",
  "学习率调整策略（lr_scheduler）": "Learning Rate Adjustment Strategy (lr_scheduler)",
  "推荐使用余弦退火cosine。如果开启预热，预热步数应该占总步数的5%-10%。": "Using cosine annealing is recommended. If using warm-up, warm-up steps should constitute 5%-10% of the total steps.",
  "如果使用带重启的余弦退火cosine_with_restarts，重启次数不应该超过4次。": "If using cosine_with_restarts, the number of restarts should not exceed 4.",
  "批次大小 （batch_size）": "Batch Size (batch_size)",
  "Batch size 越大梯度越稳定，也可以使用更大的学习率来加速收敛，但是占用显存也更大。": "Larger batch sizes lead to more stable gradients and allow for larger learning rates to expedite convergence. However, it also requires more GPU memory.",
  "一般而言 2 倍的 batch_size 可以使用两倍的 UNet 学习率，但是TE学习率不能提高太多。": "Generally, for a 2x increase in batch_size, you can use a 2x higher UNet learning rate, but avoid significantly increasing TE's learning rate.",
  "优化器": "Optimizer",
  "这里只介绍最常用的三种:": "Here, we'll introduce the three most common ones:",
  "AdamW8bit": "AdamW8bit",
  "：启用的int8优化的AdamW优化器，默认选项。": ": AdamW optimizer with int8 optimization enabled, default option.",
  "Lion": "Lion",
  "：Google Brain发表的新优化器，各方面表现优于AdamW，同时占用显存更小，可能需要更大的batch size以保持梯度更新稳定。": ": Lion, a new optimizer from Google Brain, outperforms AdamW in many aspects, uses less GPU memory, and might need a larger batch size to maintain gradient stability.",
  "D-Adaptation": "D-Adaptation",
  "：FB发表的自适应学习率的优化器，调参简单，无需手动控制学习率，但是占用显存巨大(通常需要大于8G)。使用时": ": D-Adaptation, an optimizer from Facebook AI, offers adaptive learning rates and simple tuning without manual control. However, it requires significant GPU memory (usually more than 8GB). When using it,",
  "设置学习率为1": "set the learning rate to 1",
  "即可，同时": "and also set",
  "学习率调整策略使用constant": "the learning rate adjustment strategy to constant",
  "。需要添加&quot;--optimizer_args decouple=True&quot;来分离UNet和TE的学习率。(这些设置训练UI都会帮你自动处理)": ". Add &quot;--optimizer_args decouple=True&quot; to decouple UNet and TE's learning rates. (These settings are automatically handled by the training UI.)",
  "网络设置": "Network Settings",
  "网络结构（LoRA/LoCon/LoHa/DyLoRA）": "Network Architecture (LoRA/LoCon/LoHa/DyLoRA)",
  "不同网络结构对应不同的矩阵低秩分解方法。LoRA 是老祖宗，只控制模型中的线性层和1x1卷积层，后续的不同网络结构都是在 LoRA 的基础上进行改进。": "Different network architectures correspond to different matrix low-rank decomposition methods. LoRA is the ancestor and only controls linear and 1x1 convolutional layers. Subsequent network structures build upon LoRA's foundation.",
  "LyCORIS 对其进行改进，添加了其他几种算法：": "LyCORIS improves upon this and adds a few other algorithms:",
  "LoCon 加入了对卷积层 (Conv) 的控制": "LoCon introduces control over convolutional layers (Conv)",
  "LoHa（哈达玛积）和 LoKr（克罗内克积）": "LoHa (Hadamard Product) and LoKr (Kronecker Product)",
  "IA3": "IA3",
  "理论上来说 LyCORIS 会比 LoRA 拥有更加强的微调效果，但是也更加容易过拟合。": "In theory, LyCORIS should have a stronger fine-tuning effect than LoRA, but it's also more prone to overfitting.",
  "需要注意的是，不同的网络结构一般需要对应不同的 dim 以及学习率。": "It's worth noting that different network structures typically require different dim values and learning rates.",
  "网络大小": "Network Size",
    "网络大小应该根据实际的训练集图片数量和使用的网络结构决定": "Network size should be determined based on the actual number of training set images and the used network structure.",
    "上表中值为我自己的角色训练推荐值，训练画风和概念需要适当增加 Linear 部分大小。推荐值并非对各个不同的数据集都是最优的，需要自己实验得出最优。Conv 的大小最好不要超过8。": "The values in the table above are my recommended values for character training, and the training style and concepts need an appropriately increased size for the Linear part. Recommended values are not optimal for all different datasets, and you need to experiment to find the best values. It's better not to exceed 8 for the size of Conv.",
    "网络Alpha（network_alpha）": "Network Alpha (network_alpha)",
    "alpha在训练期间缩放网络的权重，alpha越小学习越慢，关系可以认为是负线性相关的。": "Alpha scales the weights of the network during training. The smaller the alpha, the slower the learning. The relationship can be considered negatively linear. ",
    "一般设置为dim/2或者dim/4。如果选择1，则需要提高学习率或者使用D-Adapation优化器。": "It is generally set to dim/2 or dim/4. If choosing 1, you need to increase the learning rate or use the D-Adaptation optimizer.",
    "高级设置": "Advanced Settings",
    "Caption 相关": "Caption-related",
    "caption dropout": "Caption Dropout",
    "网上关于这几个caption dropout的说明少之又少，甚至作者在文档里面也没有包含这些参数，只能在代码注释里面找到说明。但是caption dropout在某些情况下对模型性能有提升，所以拿出来讲一下。": "There is very little information online about these caption dropout settings. Even the author hasn't included these parameters in the documentation; you can only find explanations in the code comments. However, caption dropout can improve model performance in some cases, so let's talk about it.",
    "caption_dropout_rate：丢弃全部标签的概率，对一个图片概率不使用caption或class token": "caption_dropout_rate: Probability of discarding all labels, probability of not using caption or class token for an image",
    "caption_dropout_every_n_epochs：每N个epoch丢弃全部标签。": "caption_dropout_every_n_epochs: Discard all labels every N epochs.",
    "caption_tag_dropout_rate：按逗号分隔的标签来随机丢弃tag的概率。": "caption_tag_dropout_rate: Probability of randomly discarding tags separated by commas.",
    "如果使用DB+标签的训练方法训练画风": "If using the DB+tag training method for training styles",
    "，推荐使用这个参数，能够有效防止tag过拟合，": "it's recommended to use this parameter to effectively prevent tag overfitting.",
    "一般选择0.2-0.5之间的值": "Usually choose a value between 0.2 and 0.5.",
    "。": ".",
    "训练人物则无需开启": "No need to enable for training characters.",
    "token 热身": "Token Warmup",
    "两个token热身相关的参数。": "Two parameters related to token warmup.",
    "token_warmup_min：最小学习的token数量，token_warmup_step： 在多少步后达到最大token数量。": "token_warmup_min: Minimum number of tokens to learn, token_warmup_step: At what step the maximum number of tokens is reached.",
    "token_warmup可以理解为另一种形式的caption dropout，但是如果不随机打乱token，则只会学习前面N个token。本人并未实测过启用这两个参数的效果，有兴趣可以自行实验。": "Token warmup can be understood as another form of caption dropout, but if tokens are not randomly shuffled, only the first N tokens will be learned. I haven't personally tested the effects of enabling these two parameters; you can experiment if interested.",
    "噪声相关": "Noise-related",
    "噪声偏移（noise_offset）": "Noise Offset (noise_offset)",
    "在训练过程中加入全局的噪声，改善图片的亮度变化范围（能生成更黑或者更白的图片）。": "Add global noise during training to improve the brightness range of images (generate darker or brighter images).",
    "如果需要开启，": "If needed,",
    "推荐设置值为0.1": "recommended setting is 0.1.",
    "，": ",",
    "同时需要增加学习步数作为网络收敛更慢的补偿": "and increase the number of training steps as compensation for slower network convergence.",
    "多分辨率/金字塔噪声 multires_noise_iterations、multires_noise_discount": "Multi-resolution/Pyramid Noise multires_noise_iterations, multires_noise_discount",
    "多分辨率/金字塔噪声相关参数。iteration设置在6-8，再高提升不大。discount设置在0.3-0.8之间，更小的值需要更多步数。": "Parameters related to multi-resolution/pyramid noise. Set iteration between 6 and 8 for minor improvements. Set discount between 0.3 and 0.8; smaller values require more steps.",
    "其他一堆参数": "Other bunch of parameters",
    "CLIP_SKIP": "CLIP_SKIP",
    "CLIP模型使用倒数第N层的输出，需要与底模使用的值保持一致，如果是基于NAI的二次元模型，应当使用2。如果是SD1.5等真实模型应当使用1。生成时也应该使用同样的值。": "The CLIP model uses the output of the N-th layer from the end. It needs to be consistent with the value used in the base model. If it's a NAI-based 2D model, use 2. For real models like SD1.5, use 1. The same value should be used during generation.",
    "Min-SNR-γ": "Min-SNR-γ",
    "发表于今年CVPR23上的一种加速扩散模型收敛的方法。不同样本批次的学习难度不同导致梯度方向不一致所以收敛慢，于是引入根据信噪比调整学习率比重。": "A method published at CVPR23 this year to accelerate the convergence of diffusion models. Different sample batches have varying learning difficulties, leading to inconsistent gradient directions and slow convergence. Therefore, it introduces adjusting the learning rate weights based on signal-to-noise ratio (SNR).",
    "设置在5左右的值": "Set around 5 for this value.",
    "是实验效果比较好的，但是注意优化器": "This value works well in experiments, but note that optimizer",
    "使用D-Adaptation的时候不适用": "is not suitable when using D-Adaptation.",
    "，因为学习率是优化器控制的。": "because the learning rate is controlled by the optimizer.",
    "数据增强相关": "Data Augmentation-related",
    "数据增强是在训练时实时对图片做变换的方法，可用于防止过拟合，能用的一共有四种: color_aug, flip_aug, face_crop_aug_range, random_crop。 其中只有翻转（flip_aug）能和cache latent兼容，因为latent可以直接翻转。": "Data augmentation is a method of applying transformations to images in real-time during training. It can be used to prevent overfitting. There are four methods available: color_aug, flip_aug, face_crop_aug_range, random_crop. Among them, only flip_aug is compatible with caching latent, as latent can be flipped directly.",
    "四种都不推荐使用": "None of these four methods are recommended.",
    "，因为裁剪图片的两种cropping方法都会导致tag对应不上。color_aug无法启用cache latent导致训练慢，得不偿失。翻转的flip_aug在图像不对称的情况下表现差，会导致无法正确生成人物不对称的特征（刘海、发饰等）。": "because both cropping methods for images will result in mismatched tags. Enabling color_aug disables cache latent and slows down training, which is not worth it. The flip_aug flip method performs poorly when the image is asymmetrical, leading to incorrect generation of asymmetric features for characters (bangs, hair accessories, etc.).",
    "max_grad_norm": "max_grad_norm",
    "限制模型更新梯度的大小，改善数值稳定性。梯度的范数超过这个值将会被缩放到这个大小，": "Limits the size of model update gradients to improve numerical stability. The norm of gradients exceeding this value will be scaled to this size.",
    "一般来说无需设置": "In general, no need to set this.",
    "gradient_accumulation_steps": "gradient_accumulation_steps",
    "梯度累积步数，用于在小显存上模拟大batch size的效果。": "Gradient accumulation steps used to simulate a large batch size on a small GPU memory.",
    "如果显存足够使用4以上的batch size就没必要启用": "If GPU memory is sufficient, there's no need to enable it for batch sizes of 4 or above.",
    "log_with、wandb_api_key": "log_with, wandb_api_key",
    "选择logger类型，可选tensorboard或者wandb。使用wandb需要指定api key。": "Choose the logger type; options are tensorboard or wandb. Using wandb requires specifying an API key.",
    "prior_loss_weight": "prior_loss_weight",
    "DB训练当中先验部分的权重，控制正则化图像的强度，论文中使用的是1的值，": "Weight of the prior part in DB training, controlling the strength of regularized images. The paper uses a value of 1.",
    "如无特殊情况无需更改": "No need to change unless there are special circumstances.",
    "debug_dataset": "debug_dataset",
    "不训练模型，仅输出训练集元数据和训练参数信息，可以用来检查各项设置是否正确。": "Do not train the model; only output training set metadata and training parameter information. Can be used to check if settings are correct.",
    "vae_batch_size": "vae_batch_size",
    "cache latent的时候VAE编码器的batch size，和训练效果无关。": "Batch size of VAE encoder when caching latent; unrelated to training effectiveness.",
    "一般来说使用2-4可以加速一点cache latent的过程": "Generally, using 2-4 can speed up the process of caching latent a bit.",
    "。因为VAE编码器本身参数量比较小，实测在Linux机器上8G的显卡也能开启4。Windows下系统占用显存较多，显存小于10G不建议开启。": "Because the parameter size of the VAE encoder itself is relatively small, it has been tested that even on a Linux machine with an 8GB GPU, setting it to 4 works. Windows tends to use more GPU memory, so it's not recommended to enable it for GPUs with less than 10GB memory.",
    "TIP": "TIP",
    "文档尚未完结~!": "The document is not finished yet!",
    "by": "by",
    "秋葉": "Akiha",
    "open in new window": "open in new window",
    "&amp;": "&",
    "Impossib1e嗨": "Impossib1e Hi",
    "感谢 Impossib1e嗨 贡献的大量文档": "Thanks to Impossib1e Hi for contributing a lot of documentation."
  }
  