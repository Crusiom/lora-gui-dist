<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.49">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <title>Training Parameters Tuning | SD Training UI</title><meta name="description" content="">
    <link rel="modulepreload" href="/assets/app.9ee7a77b.js"><link rel="modulepreload" href="/assets/params.html.120f9246.js"><link rel="modulepreload" href="/assets/params.html.c8cc13ef.js"><link rel="prefetch" href="/assets/index.html.9d7cc666.js"><link rel="prefetch" href="/assets/tagger.html.ddeabc3c.js"><link rel="prefetch" href="/assets/task.html.4e4c8633.js"><link rel="prefetch" href="/assets/tensorboard.html.37ea225e.js"><link rel="prefetch" href="/assets/index.html.fbbfced2.js"><link rel="prefetch" href="/assets/basic.html.9a9c0ad0.js"><link rel="prefetch" href="/assets/index.html.b97ec799.js"><link rel="prefetch" href="/assets/master.html.039830e1.js"><link rel="prefetch" href="/assets/tools.html.c0a4659a.js"><link rel="prefetch" href="/assets/about.html.5b0c0de9.js"><link rel="prefetch" href="/assets/settings.html.3a303daf.js"><link rel="prefetch" href="/assets/404.html.686caba0.js"><link rel="prefetch" href="/assets/index.html.210bb51d.js"><link rel="prefetch" href="/assets/tagger.html.5cd3fc97.js"><link rel="prefetch" href="/assets/task.html.2e8a4a4b.js"><link rel="prefetch" href="/assets/tensorboard.html.de10155c.js"><link rel="prefetch" href="/assets/index.html.cfa0939c.js"><link rel="prefetch" href="/assets/basic.html.a666d57b.js"><link rel="prefetch" href="/assets/index.html.1d83d6de.js"><link rel="prefetch" href="/assets/master.html.262267d5.js"><link rel="prefetch" href="/assets/tools.html.58a0fe3f.js"><link rel="prefetch" href="/assets/about.html.33233a07.js"><link rel="prefetch" href="/assets/settings.html.060dbbad.js"><link rel="prefetch" href="/assets/404.html.76cdaf6b.js"><link rel="prefetch" href="/assets/404.cc02b61c.js"><link rel="prefetch" href="/assets/layout.180fb5e6.js">
    <link rel="stylesheet" href="/assets/style.743820e1.css">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container no-navbar"><!--[--><!----><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar" data-v-2e807e1c=""><div class="el-scrollbar" data-v-2e807e1c=""><div class="el-scrollbar__wrap el-scrollbar__wrap--hidden-default" style=""><div class="el-scrollbar__view" style=""><!--[--><div class="sidebar-container" data-v-2e807e1c=""><!----><ul class="sidebar-items" data-v-2e807e1c=""><!--[--><li><a href="/" class="sidebar-item sidebar-heading" aria-label="SD-Trainer"><!--[--><!--]-->SD-Trainer<!--[--><!--]--></a><!----></li><li><a href="/lora/index.md" class="sidebar-item sidebar-heading active" aria-label="LoRA训练"><!--[--><!--]-->LoRA Training<!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a href="/lora/basic.md" class="sidebar-item" aria-label="新手"><!--[--><!--]-->Beginner<!--[--><!--]--></a><!----></li><li><a href="/lora/master.md" class="sidebar-item" aria-label="专家"><!--[--><!--]-->Expert<!--[--><!--]--></a><!----></li><li><a href="/lora/tools.md" class="sidebar-item" aria-label="工具"><!--[--><!--]-->Tools<!--[--><!--]--></a><!----></li><li><a href="/lora/params.md" class="sidebar-item active" aria-label="参数详解"><!--[--><!--]-->Parameter Explanation<!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a href="/tensorboard.md" class="sidebar-item sidebar-heading" aria-label="Tensorboard"><!--[--><!--]-->Tensorboard<!--[--><!--]--></a><!----></li><li><a href="/tagger.md" class="sidebar-item sidebar-heading" aria-label="WD 1.4 标签器"><!--[--><!--]-->WD 1.4 Tagger<!--[--><!--]--></a><!----></li><li><p tabindex="0" class="sidebar-item sidebar-heading">Other<!----></p><ul style="display:none;" class="sidebar-item-children"><!--[--><li><a href="/other/settings.md" class="sidebar-item" aria-label="UI 设置"><!--[--><!--]-->UI Settings<!--[--><!--]--></a><!----></li><li><a href="/other/about.md" class="sidebar-item" aria-label="关于"><!--[--><!--]-->About<!--[--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><ul class="sidebar-bottom" data-v-2e807e1c=""><li class="sidebar-item sidebar-heading appearance" data-v-2e807e1c="">Github<a class="icon" href="https://github.com/Akegarasu/lora-scripts" target="_blank" aria-label="GitHub" data-v-2e807e1c=""><svg xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-2e807e1c=""><path d="M12 2C6.475 2 2 6.475 2 12a9.994 9.994 0 0 0 6.838 9.488c.5.087.687-.213.687-.476c0-.237-.013-1.024-.013-1.862c-2.512.463-3.162-.612-3.362-1.175c-.113-.288-.6-1.175-1.025-1.413c-.35-.187-.85-.65-.013-.662c.788-.013 1.35.725 1.538 1.025c.9 1.512 2.338 1.087 2.912.825c.088-.65.35-1.087.638-1.337c-2.225-.25-4.55-1.113-4.55-4.938c0-1.088.387-1.987 1.025-2.688c-.1-.25-.45-1.275.1-2.65c0 0 .837-.262 2.75 1.026a9.28 9.28 0 0 1 2.5-.338c.85 0 1.7.112 2.5.337c1.912-1.3 2.75-1.024 2.75-1.024c.55 1.375.2 2.4.1 2.65c.637.7 1.025 1.587 1.025 2.687c0 3.838-2.337 4.688-4.562 4.938c.362.312.675.912.675 1.85c0 1.337-.013 2.412-.013 2.75c0 .262.188.574.688.474A10.016 10.016 0 0 0 22 12c0-5.525-4.475-10-10-10z" fill="currentColor" data-v-2e807e1c=""></path></svg></a></li><li class="sidebar-item sidebar-heading appearance" data-v-2e807e1c="">Bulb<button class="toggle-color-mode-button" title="toggle color mode" data-v-2e807e1c=""><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button></li></ul></div><!--]--></div></div><!--[--><div class="el-scrollbar__bar is-horizontal" style="display:none;"><div class="el-scrollbar__thumb" style="width:0;transform:translateX(0%);"></div></div><div class="el-scrollbar__bar is-vertical" style="display:none;"><div class="el-scrollbar__thumb" style="height:0;transform:translateY(0%);"></div></div><!--]--></div></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><h1 id="训练参数调节" tabindex="-1"><a class="header-anchor" href="#训练参数调节" aria-hidden="true"></a>Training Parameters Tuning</h1><h2 id="设置训练用模型、数据集" tabindex="-1"><a class="header-anchor" href="#设置训练用模型、数据集" aria-hidden="true">Set Training Models and Datasets</a>Base Model Selection</h2><h3 id="底模选择" tabindex="-1"><a class="header-anchor" href="#底模选择" aria-hidden="true">Base model, preferably choose ancestral models for more versatile LoRA training. Training on fusion models may result in</a>better image generation, but with reduced generalization. You can make the choice.</h3><p><strong>What are ancestral models?</strong>sd1.5 2.0, novelai original leaked models. These are non-fusion models. Fusion models like the anything series fuse many models, and orangemix series fuse anything and basil for better performance. Training on them may have worse transferability.</p><p>Training Resolution</p><p>Resolution during training</p><h3 id="训练分辨率" tabindex="-1"><a class="header-anchor" href="#训练分辨率" aria-hidden="true">width, height</a>, can be non-square, but must be a multiple of 64. It's recommended to use values greater than 512x512 and less than 1024x1024. The aspect ratio should be determined by the distribution of the training set. For instance, if long images are dominant, consider using resolutions like 512x768, or if wide images dominate, use 768x512.</h3><p>ARB Buckets<code>ARB buckets are enabled by default, allowing the use of images with non-fixed aspect ratios for training (essentially, no need for manual cropping). ARB buckets may increase training time to some extent.</code>ARB bucket resolution must be higher than the training resolution.</p><h3 id="arb-桶" tabindex="-1"><a class="header-anchor" href="#arb-桶" aria-hidden="true">Learning Rate and Optimizer Settings</a>Learning Rate Settings</h3><p>UNet and TE usually have different learning rates, as their learning difficulties differ. Generally, UNet's learning rate is higher than TE's.<strong>As shown in the graph, we aim for UNet and TE to be in an optimal range (green portion), but we're uncertain about the exact values.</strong></p><h2 id="学习率与优化器设置" tabindex="-1"><a class="header-anchor" href="#学习率与优化器设置" aria-hidden="true">Insufficient UNet training results in unfaithful images, while excessive training leads to distorted faces or excessive color patches. Inadequate TE training reduces adherence to prompts in output images, whereas excessive training generates superfluous objects.</a>Total Learning Steps = (Number of Images * Repetitions * Epochs) / Batch Size</h2><h3 id="学习率设置" tabindex="-1"><a class="header-anchor" href="#学习率设置" aria-hidden="true">Taking UNet's learning rate as 1e-4, generally, for fewer images, training characters requires at least 1000 steps, style training needs at least 2500 steps, and concept training needs at least 3000 steps. These are just minimum steps; more images require more steps. With a larger learning rate, steps can be slightly reduced. However, it's not linear; using double the learning rate requires more than half the previous steps.</a>The best way to determine learning rates and steps is to train first, then test. Good initial values are typically 1e-4 for UNet and 5e-5 for TE.</h3><p>Learning Rate Adjustment Strategy (lr_scheduler)</p><p><img src="https://pic5.58cdn.com.cn/nowater/webim/big/n_v2380202141f23436d94bd3b1fa26bfbad.png" alt="">Using cosine annealing is recommended. If using warm-up, warm-up steps should constitute 5%-10% of the total steps.</p><p>If using cosine_with_restarts, the number of restarts should not exceed 4.</p><p><strong>Batch Size (batch_size)</strong></p><p>Larger batch sizes lead to more stable gradients and allow for larger learning rates to expedite convergence. However, it also requires more GPU memory.</p><p><strong>Generally, for a 2x increase in batch_size, you can use a 2x higher UNet learning rate, but avoid significantly increasing TE's learning rate.</strong></p><h3 id="学习率调整策略-lr-scheduler" tabindex="-1"><a class="header-anchor" href="#学习率调整策略-lr-scheduler" aria-hidden="true">Optimizer</a>Here, we'll introduce the three most common ones:</h3><p>AdamW8bit</p><p>: AdamW optimizer with int8 optimization enabled, default option.</p><h3 id="批次大小-batch-size" tabindex="-1"><a class="header-anchor" href="#批次大小-batch-size" aria-hidden="true">Lion</a>: Lion, a new optimizer from Google Brain, outperforms AdamW in many aspects, uses less GPU memory, and might need a larger batch size to maintain gradient stability.</h3><p>D-Adaptation</p><p>: D-Adaptation, an optimizer from Facebook AI, offers adaptive learning rates and simple tuning without manual control. However, it requires significant GPU memory (usually more than 8GB). When using it,</p><h3 id="优化器" tabindex="-1"><a class="header-anchor" href="#优化器" aria-hidden="true">set the learning rate to 1</a>and also set</h3><p>the learning rate adjustment strategy to constant</p><ul><li><strong>. Add &quot;--optimizer_args decouple=True&quot; to decouple UNet and TE's learning rates. (These settings are automatically handled by the training UI.)</strong>Network Settings</li><li><strong>Network Architecture (LoRA/LoCon/LoHa/DyLoRA)</strong>Different network architectures correspond to different matrix low-rank decomposition methods. LoRA is the ancestor and only controls linear and 1x1 convolutional layers. Subsequent network structures build upon LoRA's foundation.</li><li><strong>LyCORIS improves upon this and adds a few other algorithms:</strong>LoCon introduces control over convolutional layers (Conv)<strong>LoHa (Hadamard Product) and LoKr (Kronecker Product)</strong>IA3<strong>In theory, LyCORIS should have a stronger fine-tuning effect than LoRA, but it's also more prone to overfitting.</strong>It's worth noting that different network structures typically require different dim values and learning rates.</li></ul><h2 id="网络设置" tabindex="-1"><a class="header-anchor" href="#网络设置" aria-hidden="true">Network Size</a>Network size should be determined based on the actual number of training set images and the used network structure.</h2><h3 id="网络结构-lora-locon-loha-dylora" tabindex="-1"><a class="header-anchor" href="#网络结构-lora-locon-loha-dylora" aria-hidden="true">The values in the table above are my recommended values for character training, and the training style and concepts need an appropriately increased size for the Linear part. Recommended values are not optimal for all different datasets, and you need to experiment to find the best values. It's better not to exceed 8 for the size of Conv.</a>Network Alpha (network_alpha)</h3><p>Alpha scales the weights of the network during training. The smaller the alpha, the slower the learning. The relationship can be considered negatively linear. </p><p>It is generally set to dim/2 or dim/4. If choosing 1, you need to increase the learning rate or use the D-Adaptation optimizer.</p><ul><li>Advanced Settings</li><li>Caption-related</li><li>Caption Dropout</li></ul><p>There is very little information online about these caption dropout settings. Even the author hasn't included these parameters in the documentation; you can only find explanations in the code comments. However, caption dropout can improve model performance in some cases, so let's talk about it.</p><p>caption_dropout_rate: Probability of discarding all labels, probability of not using caption or class token for an image</p><h3 id="网络大小" tabindex="-1"><a class="header-anchor" href="#网络大小" aria-hidden="true">caption_dropout_every_n_epochs: Discard all labels every N epochs.</a>caption_tag_dropout_rate: Probability of randomly discarding tags separated by commas.</h3><p>If using the DB+tag training method for training styles</p><p><img src="https://pic1.58cdn.com.cn/nowater/webim/big/n_v208559ede41484049ada738fbdd67d047.jpg" alt="network_dim"></p><p>it's recommended to use this parameter to effectively prevent tag overfitting.</p><h3 id="网络alpha-network-alpha" tabindex="-1"><a class="header-anchor" href="#网络alpha-network-alpha" aria-hidden="true">Usually choose a value between 0.2 and 0.5.</a>.</h3><p>No need to enable for training characters.</p><p>Token Warmup</p><h2 id="高级设置" tabindex="-1"><a class="header-anchor" href="#高级设置" aria-hidden="true">Two parameters related to token warmup.</a>token_warmup_min: Minimum number of tokens to learn, token_warmup_step: At what step the maximum number of tokens is reached.</h2><h3 id="caption-相关" tabindex="-1"><a class="header-anchor" href="#caption-相关" aria-hidden="true">Token warmup can be understood as another form of caption dropout, but if tokens are not randomly shuffled, only the first N tokens will be learned. I haven't personally tested the effects of enabling these two parameters; you can experiment if interested.</a>Noise-related</h3><h4 id="caption-dropout" tabindex="-1"><a class="header-anchor" href="#caption-dropout" aria-hidden="true">Noise Offset (noise_offset)</a>Add global noise during training to improve the brightness range of images (generate darker or brighter images).</h4><p>If needed,</p><p>recommended setting is 0.1.</p><p>,</p><p>and increase the number of training steps as compensation for slower network convergence.<strong>Multi-resolution/Pyramid Noise multires_noise_iterations, multires_noise_discount</strong>Parameters related to multi-resolution/pyramid noise. Set iteration between 6 and 8 for minor improvements. Set discount between 0.3 and 0.8; smaller values require more steps.<strong>Other bunch of parameters</strong>CLIP_SKIP<strong>The CLIP model uses the output of the N-th layer from the end. It needs to be consistent with the value used in the base model. If it's a NAI-based 2D model, use 2. For real models like SD1.5, use 1. The same value should be used during generation.</strong>Min-SNR-γ</p><h4 id="token-热身" tabindex="-1"><a class="header-anchor" href="#token-热身" aria-hidden="true">A method published at CVPR23 this year to accelerate the convergence of diffusion models. Different sample batches have varying learning difficulties, leading to inconsistent gradient directions and slow convergence. Therefore, it introduces adjusting the learning rate weights based on signal-to-noise ratio (SNR).</a>Set around 5 for this value.</h4><p>This value works well in experiments, but note that optimizer</p><p>is not suitable when using D-Adaptation.</p><p>because the learning rate is controlled by the optimizer.</p><h3 id="噪声相关" tabindex="-1"><a class="header-anchor" href="#噪声相关" aria-hidden="true">Data Augmentation-related</a>Data augmentation is a method of applying transformations to images in real-time during training. It can be used to prevent overfitting. There are four methods available: color_aug, flip_aug, face_crop_aug_range, random_crop. Among them, only flip_aug is compatible with caching latent, as latent can be flipped directly.</h3><h4 id="噪声偏移-noise-offset" tabindex="-1"><a class="header-anchor" href="#噪声偏移-noise-offset" aria-hidden="true">None of these four methods are recommended.</a>because both cropping methods for images will result in mismatched tags. Enabling color_aug disables cache latent and slows down training, which is not worth it. The flip_aug flip method performs poorly when the image is asymmetrical, leading to incorrect generation of asymmetric features for characters (bangs, hair accessories, etc.).</h4><p>max_grad_norm</p><p>Limits the size of model update gradients to improve numerical stability. The norm of gradients exceeding this value will be scaled to this size.<strong>In general, no need to set this.</strong>gradient_accumulation_steps<strong>Gradient accumulation steps used to simulate a large batch size on a small GPU memory.</strong>If GPU memory is sufficient, there's no need to enable it for batch sizes of 4 or above.</p><h4 id="多分辨率-金字塔噪声-multires-noise-iterations、multires-noise-discount" tabindex="-1"><a class="header-anchor" href="#多分辨率-金字塔噪声-multires-noise-iterations、multires-noise-discount" aria-hidden="true">log_with, wandb_api_key</a>Choose the logger type; options are tensorboard or wandb. Using wandb requires specifying an API key.</h4><p>prior_loss_weight</p><h3 id="其他一堆参数" tabindex="-1"><a class="header-anchor" href="#其他一堆参数" aria-hidden="true">Weight of the prior part in DB training, controlling the strength of regularized images. The paper uses a value of 1.</a>No need to change unless there are special circumstances.</h3><ul><li><p><strong>debug_dataset</strong>Do not train the model; only output training set metadata and training parameter information. Can be used to check if settings are correct.</p></li><li><p><strong>vae_batch_size</strong>Batch size of VAE encoder when caching latent; unrelated to training effectiveness.<strong>Generally, using 2-4 can speed up the process of caching latent a bit.</strong>Because the parameter size of the VAE encoder itself is relatively small, it has been tested that even on a Linux machine with an 8GB GPU, setting it to 4 works. Windows tends to use more GPU memory, so it's not recommended to enable it for GPUs with less than 10GB memory.<strong>TIP</strong>The document is not finished yet!</p></li><li><p><strong>by</strong>Akiha<strong>open in new window</strong>&</p></li><li><p><strong>Impossib1e Hi</strong>Thanks to Impossib1e Hi for contributing a lot of documentation.<strong></strong></p></li><li><p><strong></strong><strong></strong></p></li><li><p><strong></strong></p></li><li><p><strong></strong><strong></strong></p></li><li><p><strong></strong></p></li><li><p><strong></strong><strong></strong></p></li></ul><div class="custom-container tip"><p class="custom-container-title"></p><p></p><p><a href="https://space.bilibili.com/12566101" target="_blank" rel="noopener noreferrer"><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only"></span></span></a><a href="https://space.bilibili.com/1713054" target="_blank" rel="noopener noreferrer"><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only"></span></span></a></p><p></p></div></div><!--[--><!--]--></div><footer class="page-meta"><!----><!----><!----></footer><!----><!--[--><!--]--></main><!--]--></div><!----><!--]--></div>
    <script type="module" src="/assets/app.9ee7a77b.js" defer=""></script>
  </body>
</html>
